# Kafka-Based Image Classification System

This project demonstrates a **production-style, event-driven architecture** for high-throughput image classification using **Apache Kafka**.

It is designed as a **learning + portfolio project** that shows:

* Event-driven design
* Horizontal scalability
* Async ML inference
* Real-world Kafka usage
* Dockerized microservices

---

## ğŸ§  High-Level Architecture

```
Frontend (Next.js)
      â†“ HTTP
Ingestion API (Node.js)
      â†“ Kafka (produce)
Kafka Topic: image.classification.requested
      â†“ Kafka (consume)
ML Inference Service (Python)
      â†“ Kafka (produce)
Kafka Topic: image.classification.completed
      â†“ Kafka (consume)
Results Service (Node.js)
      â†“ HTTP
Frontend polls results
```

---

## ğŸ§© Services Overview

### 1. Ingestion API (Node.js)

* Accepts image uploads
* Generates a `jobId`
* Publishes classification request events to Kafka
* Stateless â†’ horizontally scalable

### 2. ML Inference Service (Python)

* Kafka consumer group
* Performs image classification
* Publishes completed / failed events
* Throughput scales with partitions + replicas

### 3. Results Service (Node.js)

* Consumes inference results
* Stores job status in-memory (can be replaced with Redis/DB)
* Exposes polling endpoint

### 4. Kafka

* Message backbone
* Enables decoupling, buffering, and scaling

### 5. Frontend (Next.js)

* Uploads images
* Polls results endpoint
* Displays job status and output

---

## ğŸ—‚ï¸ Repository Structure

```
root/
â”œâ”€ docker-compose.yml
â”œâ”€ ingestion-api/
â”‚  â”œâ”€ Dockerfile
â”‚  â”œâ”€ src/
â”‚  â””â”€ package.json
â”œâ”€ ml-inference/
â”‚  â”œâ”€ Dockerfile
â”‚  â”œâ”€ app/
â”‚  â”‚  â”œâ”€ main.py
â”‚  â”‚  â”œâ”€ consumer.py
â”‚  â”‚  â”œâ”€ producer.py
â”‚  â”‚  â””â”€ config.py
â”‚  â””â”€ requirements.txt
â”œâ”€ results-service/
â”‚  â”œâ”€ Dockerfile
â”‚  â”œâ”€ src/
â”‚  â””â”€ package.json
â””â”€ frontend/
   â””â”€ Next.js app
```

---

## âš™ï¸ Prerequisites

* Docker
* Docker Compose
* Node.js (for local dev, optional)
* Python 3.10+ (for local dev, optional)

---

## ğŸš€ Running the System (Docker)

### 1ï¸âƒ£ Clone the repository

```bash
git clone <repo-url>
cd <repo>
```

### 2ï¸âƒ£ Build all services

```bash
docker compose build
```

### 3ï¸âƒ£ Start the system

```bash
docker compose up
```

This will start:

* Kafka + Zookeeper
* Ingestion API â†’ `http://localhost:3000`
* Results Service â†’ `http://localhost:4000`
* Frontend â†’ `http://localhost:3001`
* ML Inference Service (background consumer)

---

## ğŸ” Verifying the Setup

### Ingestion API

```bash
curl -X POST http://localhost:3000/api/classify
```

Response:

```json
{
  "jobId": "uuid",
  "status": "accepted"
}
```

### Results API

```bash
curl http://localhost:4000/api/results/<jobId>
```

Response states:

* `pending`
* `completed`
* `failed`

---

## ğŸ“ˆ Scaling the System

### Scale ingestion API

```bash
docker compose up --scale ingestion-api=3
```

### Scale ML inference workers

```bash
docker compose up --scale ml-inference=4
```

âš ï¸ Ensure Kafka topic partitions â‰¥ ML replicas.

---

## ğŸ“Š Observability (Basic)

The system logs:

* Request rate
* Kafka produce latency
* Inference duration
* End-to-end job latency

Kafka consumer lag can be checked via:

```bash
kafka-consumer-groups --bootstrap-server kafka:9092 \
  --describe --group ml-inference-group
```

---

## ğŸ§ª Stress Testing

Example load test on ingestion API:

```bash
autocannon -c 50 -d 30 http://localhost:3000/api/classify
```

Observe:

* Kafka lag
* Inference throughput
* End-to-end latency

---

## ğŸ§± Design Principles Used

* Event-driven architecture
* Open/Closed principle
* Async processing
* Backpressure via Kafka
* Horizontal scalability

---

## ğŸ”® Future Improvements

* Redis for results storage
* Server-Sent Events / WebSockets
* Prometheus + Grafana
* Batch inference
* GPU-aware scheduling
* Dead-letter topics

---

Happy hacking ğŸš€
